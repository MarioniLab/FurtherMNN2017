---
title: A description of the theory behind the `fastMNN` algorithm
author: Aaron Lun
date: "`r Sys.Date()`"
output:
  BiocStyle::html_document:
    toc_float: yes
---

# Overview

The `fastMNN()` approach is much simpler than the original `mnnCorrect()` algorithm, and proceeds in several steps.

1. Perform a multi-sample PCA on the (cosine-)normalized expression values to reduce dimensionality.
2. Identify MNN pairs in the low-dimensional space between a reference batch and a target batch.
3. Remove variation along the average batch vector in both reference and target batches.
4. Correct the cells in the target batch towards the reference, using locally weighted correction vectors.
5. Merge the corrected target batch with the reference, and repeat with the next target batch.

# Cosine normalization

## Rationale

If scaling normalization is only performed _within_ each batch, there may be a systematic scaling difference in the normalized expression values between batches.
This unnecessarily adds an extra dimension to the differences between batches, which complicates later correction steps.

Scaling differences are best avoided by ensuring that the size factors bring all batches to the same scale, which is the purpose of `multiBatchNorm()`.
This function downscales all other batches to the same scale as the lowest-coverage batch prior to addition of a pseudo-count (of unity) and log-transformation.
The use of downscaling mitigates differences in variance at low-coverage genes, by increasing the impact of the pseudo-count to shrink all log-expression values towards zero.

So, ideally, one would address this problem during normalization.
However, this is not possible if we consider `fastMNN()` in isolation with arbitrary log-expression values.
Accurately reversing the log-transformation requires knowledge of the base of the log function and the added pseudo-count, 
which complicates the interface at best and may not be available at worst.

Cosine normalization of the log-expression values represents the next-best solution for removing these differences between batches.
Real single-cell data contains many small counts (where the log function is near-linear) or many zeroes (which remain zero when the pseudo-count is 1).
In these applications, scaling differences due to separate normalization will not manifest as the expected shift in the log-transformed expression.
Rather, they are better represented as scaling differences in the log-expression vectors, which cosine normalization aims to remove.

## Preserving orthogonality

If the batch effect was orthogonal to the biological subspace in the un-normalized space, is this still the case after cosine normalization?
Not in general - one can imagine a simple 2D scenario where the biological subspace is a horizontal line and the batch effect is a vertial shift.
Cosine normalization will map the manifolds onto the unit circle around the origin and break orthogonality.

We can consider the cases where this discrepancy is minimized. 
If we have the biological subspace $B$ and a batch vector $W$, the cosine-normalized expression without the batch vector is

$$
\frac{\mathbf{B}x}{\sqrt{x^T\mathbf{B}^T\mathbf{B}x}} \;,
$$

for a cell corresponding to $x$.
The cosine-normalized expression after adding the batch vector is

$$
\frac{\mathbf{B}x + W}{\sqrt{(\mathbf{B}x + W)^T(\mathbf{B}x + W)}} \;.
$$

The difference between the two represents the batch vector in cosine-normalized space. 
Biological differences in cosine-normalized space can be represented by differences between the cell corresponding to $x$ and another cell corresponding to $y$, i.e.,

$$
\frac{\mathbf{B}x}{\sqrt{x^T\mathbf{B}^T\mathbf{B}x}} - \frac{\mathbf{B}y}{\sqrt{y^T\mathbf{B}^T\mathbf{B}y}} 
$$

for any $y\ne x$.
We ask whether the batch vector is orthogonal to an arbitrary biological difference vector, i.e., we look at

$$
\begin{align*}
& \left(\frac{\mathbf{B}x}{\sqrt{x^T\mathbf{B}^T\mathbf{B}x}} - \frac{\mathbf{B}y}{\sqrt{y^T\mathbf{B}^T\mathbf{B}y}} \right)^T
\left(\frac{\mathbf{B}x + W}{\sqrt{(\mathbf{B}x + W)^T(\mathbf{B}x + W)}} - \frac{\mathbf{B}x}{\sqrt{x^T\mathbf{B}^T\mathbf{B}x}} \right) \\
& = \left(\frac{x^T\mathbf{B}^T\mathbf{B}x}{\sqrt{x^T\mathbf{B}^T\mathbf{B}x}} - \frac{y^T\mathbf{B}^T\mathbf{B}x}{\sqrt{y^T\mathbf{B}^T\mathbf{B}y}} \right)
\left[ \frac{1}{\sqrt{x^T\mathbf{B}^T\mathbf{B}x + W^TW}} - \frac{1}{\sqrt{x^T\mathbf{B}^T\mathbf{B}x}} \right] \;,
\end{align*}
$$

which is only close to zero for arbitrary $y$ when $W^TW$ is small relative to $x^T\mathbf{B}^T\mathbf{B}x$.
In other words, the batch effect remains approximately orthogonal if the batches are not too far apart relative to the L2 norm of the expression vector.
This is probably reasonable if the batch effect is not as large as the magnitude of expression for each gene.

Of course, this is all rather academic, as one can simply change the orthogonality assumption to refer to the cosine-normalized space in the first place.
The cosine-normalized space loses one dimension of variation relative to the un-normalized space, but both are still high-dimensional,
so the assumption of orthogonality is still reasonable in the former.

# Reducing dimensionality

## Performing an multi-batch PCA

We first perform a PCA across all cells in all batches to reduce dimensionality.
This decreases the size of the data for further analysis - in particular, improving the speed of nearest-neighbour detection.
It also removes high-dimensional technical noise that can interfere with nearest-neighbour detection.
We stress that this step has no role in the batch correction itself, and indeed, we would expect the first few PCs to be dominated by the batch effect.
We also note that this does not compromise the validity of the MNN approach, which is based on distances between cells.
Provided enough PCs are taken (default `d=50`), the distances between cells in the PC space can approximate well the distances in the original space.

The procedure itself is as conceptually simple as `cbind`ing all datasets together and performing a PCA on the merged dataset.
The only modification to the procedure is to ensure that each batch contributes equally to the basis vectors.
Specifically, the mean vector used for centering is defined as the grand mean of the batch-specific mean vectors;
and the contribution of each batch to the gene-gene covariance matrix is divided by the number of cells in each batch.
This ensures that batches with large numbers of cells do not dominate the PCA calculations.
All cells are then projected into the PC space using the identified basis vectors.

## Considerations of orthogonality

An interesting question is whether orthogonality is preserved in the low-dimensional subspace.
Consider the batch effect vector $W$ and the biological subspace defined by the column vectors of $\mathbf{B}$ (for simplicity, we will assume that all vectors are unit length).
In the original expression space, we have assumed that $W$ is orthogonal to $\mathbf{B}$, i.e., $W^T\mathbf{B} = 0$.
This ensures that MNN pairs are correctly identified between corresponding subpopulations in different batches, as described in the original paper.

Now, consider a projection matrix $\mathbf{V}_k$ from the PCA, corresponding to the first $k$ PCs.
The key assumption is that $\mathbf{V}_k$ "captures" the entirety of $W$, i.e., 
there is some linear combination $A$ of the column vectors of $\mathbf{V}_k$ such that $\mathbf{V}_{k} A =W$.
Note that this means that $A = \mathbf{V}_k^TW$, due to the orthonormality of the column vectors of $\mathbf{V}_k$.

If we project everything into the subspace defined by $\mathbf{V}_k$, the rotated biological subspace becomes $\mathbf{B}_k = \mathbf{V}^T_k \mathbf{B}$.
The rotated batch vector becomes $W_k = \mathbf{V}_k^TW$, which is equal to $A$.
This means that 

$$
W_k^T \mathbf{B}_k = A^T \mathbf{V}_k^T \mathbf{B} = (\mathbf{V}_k A)^T \mathbf{B} = W^T\mathbf{B} =  0 \;,
$$

i.e., orthogonality is preserved in the PC space.
This means that it is valid to identify MNN pairs in the PC space, provided the assumption above holds.

# Identifying MNN pairs

The use of mutually nearest neighbours is largely the same as described in the original manuscript and for `mnnCorrect()`.
As before, MNNs will be identified between cells of the same type or state across batches, assuming that the batch effect is orthogonal.
The choice of `k` should be driven by the minimum subpopulation size in each batch, with a default of `k=20`.
The only modification is that nearest neighbour identification is now performed on the PC space rather than the original gene expression space.

Of course, it is (again) possible that MNNs are incorrectly identified between cells of different type or state across batches.
This would require the presence of unique subpopulations in each batch,
which are closer to each other than to subpopulations that are shared across batches.
Such a scenario seems somewhat unfortunate.
Even with manual curation, it would be difficult to consider these subpopulations as distinct cell types in the presence of an arbitrary batch effect.

# Removing intra-batch variation

## Orthogonalization to the batch vector

Once MNN pairs are identified, the correction vector for each paired cell in the target batch is computed.
If a paired cell is involved in multiple MNN pairs, its correction vector is defined as an average across all of its pairs.
The average batch vector is then computed by averaging across the correction vectors for all paired cells.
This represents an estimate of the overall batch effect.

We then project all cells in the target batch onto the average batch vector, yielding a per-cell component in the direction of the average batch vector.
Any variation in the components represents uninteresting technical noise, assuming orthogonality between batch and biological effects.
This is eliminated by adjusting the cell coordinates so that the components of all cells are equal to the mean value within the target batch.
We repeat this for the reference batch.

Note that this step is _not_ the batch correction, we are simply removing variation within each batch.
The aim is to avoid the "kissing" problem for dense subpopulations, whereby MNNs are only identified on the surface of each subpopulation.
In such cases, subsequent correction will fail to fully merge subpopulations as the correction vectors only bring the surfaces into contact.
By removing variation along the batch vector, we can avoid this problem as the subpopulations no longer have any "width" in either batch.

That said, the use of the average batch vector assumes that the same batch effect is present at all subpopulations.
This ignores variation in the batch effects across subpopulations, so some kissing may still be expected when this variation is present.

## Estimating the variance removed

`fastMNN()` will also compute the percentage of variance removed by this orthogonalization procedure.
This is done for both the target and reference batches.
If a high percentage of variance is removed, this suggests that there is biological structure that is parallel to the average batch vector.
Orthogonalization will subsequently remove this structure, which would not be appropriate.
In this manner, we can use the percentage of variance removed as a diagnostic for the orthogonality assumptions of the MNN procedure.

Of course, what constitutes a "large" loss of variance is debatable.
A large percentage of lost variance may be harmless (and even beneficial) if it removes uninteresting noise.
A small percentage may be damaging if it removes a rare population.
Nonetheless, this diagnostic provides a quick method for flagging problematic batches.

If one is concerned about any loss of variation due to orthogonalization,
we suggest comparing the merged analysis to separate analyses with individual batches.
Loss of important variation should manifest as, e.g., co-clustering in the merged analysis of very different cells.

## Behaviour in the absence of a batch effect

An interesting consequence of the orthogonalization step is that `fastMNN()` may not work correctly in the _absence_ of a batch effect.
In such cases, the batch vector will be of near-zero length in some random direction.
If this is parallel to the biological subspace, orthogonalization may subsequently end up removing geniune biology.
This is a natural side-effect of the orthogonality assumption, which obviously fails if there is no batch vector in the first place.
In practice, this is unlikely to be a major problem as a random vector is still likely to be orthogonal to any one biological dimension.
If this is not the case, we should be able to observe a large loss of variance that indicates that `fastMNN()` should not be run.

`fastMNN()` can also be instructed to skip the correction if the relative batch effect size is below some threshold.
The relative size is defined as the ratio of the L2 norm of the average correction vector to the expected L2 norm of the per-pair vectors.
This is small if there is no batch effect as the per-pair vectors will point in different directions. 
If large losses of variance at particular merge steps are suspected to be caused by the lack of a batch effect,
we recommend examining the relative effect sizes and picking a threshold that allows one to skip those steps.

**Comments:**

- Another strategy would be to ask whether or not the average batch effect size is significantly greater than zero.
However, this requires explicit distributional assumptions (e.g., i.i.d. normal) that are unlikely to be reasonable.
We are also more likely to reject the null with more cells, eventually favouring correction even in the presence of tiny batch effects.
In contrast, our definition of the relative effect size will simply give a more precise estimate as the number of cells increases.
- `fastMNN()` will not skip correction by default, even though the relative effect sizes are available.
This is because the relative effect size can be small in situations where there is a genuine batch effect
(e.g., due to a small proportion of per-pair vectors that are very large).
More generally, the absence of a batch effect is an uncommon scenario that warrants some further manual investigation.

# Performing the batch correction

For each cell $i$ in the target batch, we identify the `k` nearest neighbouring paired cells, i.e., cells in the same batch that are involved in a MNN pair.
The correction vector for cell $i$ is defined as a locally weighted average of the correction vector of the neighbouring paired cells.
The weighting is done using a tricube scheme, where the bandwidth is defined as `ndist=3` times the median distance to the `k` neighbours.
This favours neighbours that are closer to $i$ and provides some robustness against cells in different subpopulations (e.g., if subpopulation to which $i$ belongs is small).

Cells in the target batch are then batch-corrected by subtracting the correction vector from the coordinates in the PC space.
The newly corrected cells are merged with the reference batch, and the entire process is repeated with a new batch.
Note that the PCA step is only done once at the start, though.

# Further comments

## Using `multiBatchNorm()`

Previously, `mnnCorrect()` relied solely on the cosine normalization to adjust for differences in coverage between batches.
This was not ideal as the cosine normalization operated on the log-space, and would not correctly adjust for scaling differences in the raw space.
To provide better inputs to `mnnCorrect()`, `r Biocpkg("scran")` now contains the `multiBatchNorm()` function.
This accepts a number of batches and will downscale the counts in each batch (indirectly via adjustment of size factors) to match the coverage of the lowest-coverage batch.
Subsequent normalization and log-transformation will then yield expression values that are more directly comparable between batches.

We deliberately chose to downscale all batches rather than scale all batches to the mean or median coverage.
By downscaling, we increase the shrinkage of log-expression values towards zero upon addition of the pseudo-count of 1 in `normalize()`.
This suppresses batch-to-batch differences in the technical noise at low counts and improves the quality of the MNN correction.
Otherwise, cells in low-coverage batches would be more spread out, making it difficult to identify the correct MNN pairs.

Obviously, biological variation will also experience greater shrinkage towards zero. 
However, this seems to be less of an issue as large counts in upregulated genes are less affected by shrinkage.
In addition, technical noise at large counts is less pronounced, so reduced shrinkage is not a problem.
We note that standardization of the expression matrix is not advisable here as batches with different cell type composition will geniunely exhibit different per-gene variances.

## Computational performance 

Using a single core, the `fastMNN()` function completes in 17 minutes on merging the 68K PBMC droplet dataset with the 4K T cell dataset.
Most of the time is taken up by computing the cross-product for the initial PCA, which can be (but is not yet) easily parallelized.
Memory usage is minimal through the use of the `r Biocpkg("DelayedArray")` framework, which avoids creating the merged matrix explicitly for PCA.
The nearest neighbour search is performed using the `r Biocpkg("BiocNeighbors")` package, which provides a speed boost over conventional KD-trees for high-dimensional data (see https://github.com/LTLA/OkNN2018) and also supports parallelization.

# Known limitations

The key to the performance of any MNN correction method lies in the ability to detect correct MNNs.
This is not always straightforward.
When the batch effect is not orthogonal, noise in the biological subspace can cause the wrong MNNs to be identified between the "surfaces" of two cell populations.
In such cases, orthogonalization is not effective at solving the kissing problem.
The solution here is to increase `k` to allow the correct MNNs to be identified beyond the population surface.

A more troubling case involves detection of MNNs between two unique populations in different batches, as discussed above.
This would result in the merging of two distinct subpopulations (i.e., that are genuinely separated on the biological subspace) after batch correction.

- At first glance, this seems like an incorrect outcome, but the actual interpretation varies.
One can imagine that the separation in the biological subspace represents a batch effect, 
either technical in nature due to non-orthogonality or due to some uninteresting biological effect (e.g., genetics, sex, age).
In such cases, an aggressive merge would be a desirable result.
- If such a merge is definitely undesirable, it is possible to modify the algorithm to avoid it.
This is achieved by only considering the component of the correction vector for each cell that is parallel to the average batch vector.
Here, the assumption is that most cells are shared between batches and have the same batch effect - any deviations from the average vector are biological and should be preserved.
However, this is likely to degrade the performance of the correction in the general case where there is any locality in the batch effect.
- In any case, if the merge is undesirable and has still occurred, it is possible to recover the differences with an examination of the original expression values.
One can see if there is any differential expression across conditions that reflect the differences in the two populations along the biological subspace.
This usually requires replication to distinguish between the random batch effect and genuine biological differences.


